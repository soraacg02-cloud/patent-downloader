import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import zipfile
import io
import random
import urllib.parse
import re

st.set_page_config(page_title="å°ˆåˆ© PDF çµ‚æ¥µä¸‹è¼‰å™¨ v5.0", page_icon="ğŸš€")

st.title("ğŸš€ Google Patents çµ‚æ¥µä¸‹è¼‰å™¨ v5.0")
st.markdown("""
**åŠŸèƒ½æ›´æ–°ï¼š**
1. é‡å° `18/671705` ç­‰ç”³è«‹è™Ÿé€²è¡Œå¼·åŒ–æœå°‹ã€‚
2. é¡¯ç¤ºæœå°‹åˆ°çš„ã€ŒçœŸå¯¦èº«åˆ†ã€æ¡ˆè™Ÿã€‚
""")

# 1. ä½¿ç”¨è€…è¼¸å…¥å€
patent_ids = st.text_area(
    "åœ¨æ­¤è¼¸å…¥å°ˆåˆ©æ¡ˆè™Ÿ (ä¸€è¡Œä¸€å€‹)", 
    height=150, 
    placeholder="18/671705 (ç”³è«‹è™Ÿ)\nUS20240088000A1 (å…¬é–‹è™Ÿ)"
)

def get_headers():
    """éš¨æ©Ÿåˆ‡æ›èº«åˆ†ï¼Œé¿å…è¢« Google èªå®šæ˜¯æ©Ÿå™¨äºº"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) Chrome/118.0.0.0 Safari/537.36"
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/"
    }

def search_google_for_correct_url(query):
    """
    ç•¶ç›´æ¥ä¸‹è¼‰å¤±æ•—æ™‚ï¼Œå» Google æœå°‹ã€ŒçœŸå¯¦æ¡ˆè™Ÿã€
    """
    # é‡å°ç”³è«‹è™Ÿçš„ç‰¹æ®Šå„ªåŒ–ï¼šåŠ ä¸Š "patent" é—œéµå­—è®“ Google çŸ¥é“æˆ‘å€‘åœ¨æ‰¾å°ˆåˆ©
    search_query = f"{query} patent site:patents.google.com"
    google_url = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
    
    try:
        # éš¨æ©Ÿå»¶é²ï¼Œæ¨¡æ“¬çœŸäººæ€è€ƒ
        time.sleep(random.uniform(1.5, 3.0))
        resp = requests.get(google_url, headers=get_headers(), timeout=10)
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # å°‹æ‰¾æœå°‹çµæœä¸­çš„é€£çµ
            # Google çš„æœå°‹çµæœé€šå¸¸åŒ…å«åœ¨ <a href="..."> ä¸­ï¼Œä¸”é€£çµæŒ‡å‘ patents.google.com/patent/
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                if "patents.google.com/patent/" in href:
                    # æ¸…ç†é€£çµ (æœ‰æ™‚å€™æœƒåŒ…å« /url?q=...)
                    if "/url?q=" in href:
                        href = href.split("/url?q=")[1].split("&")[0]
                    return href
    except Exception as e:
        print(f"Search Error: {e}")
    return None

def get_pdf_data(patent_id):
    """
    ä¸»é‚è¼¯ï¼š
    1. å˜—è©¦ç›´æ¥çŒœæ¸¬ (å¿«)
    2. å¤±æ•—å‰‡å» Google æœå°‹ (æ…¢ä½†æº–)
    3. ä¸‹è¼‰ PDF
    """
    clean_id = patent_id.strip()
    status_msg = ""
    
    # --- éšæ®µä¸€ï¼šç²å–æ­£ç¢ºçš„ç¶²å€ ---
    target_url = None
    
    # 1. å…ˆè©¦è©¦çœ‹ç›´æ¥æ‹¼ç¶²å€ (é©åˆæ¨™æº–å…¬é–‹è™Ÿ)
    guess_url = f"https://patents.google.com/patent/{clean_id.replace('/', '').replace('-', '')}/en"
    try:
        if requests.get(guess_url, headers=get_headers(), timeout=5).status_code == 200:
            target_url = guess_url
    except:
        pass

    # 2. å¦‚æœç›´é€£å¤±æ•—ï¼Œå•Ÿå‹• Google æœå°‹ (é©åˆç”³è«‹è™Ÿ 18/671705)
    if not target_url:
        found_url = search_google_for_correct_url(clean_id)
        if found_url:
            target_url = found_url
            status_msg = f"(é€éæœå°‹æ‰¾åˆ°å°æ‡‰ç¶²é )"

    if not target_url:
        return "NOT_FOUND", None, None, "æ‰¾ä¸åˆ°å°æ‡‰ç¶²é "

    # --- éšæ®µäºŒï¼šå¾ç¶²é ä¸­æŠ“ PDF ---
    try:
        resp = requests.get(target_url, headers=get_headers(), timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # æŠ“å–çœŸå¯¦æ¡ˆè™Ÿ (å¾ç¶²é æ¨™é¡Œæˆ–ç¶²å€åˆ†æ)
        real_id = target_url.split("/patent/")[-1].split("/")[0]
        
        pdf_link = None
        # æ–¹æ³• A: æ‰¾é€£çµ
        for link in soup.find_all('a', href=True):
            if link['href'].endswith('.pdf'):
                pdf_link = link['href']
                break
        
        # æ–¹æ³• B: æ‰¾ Meta æ¨™ç±¤
        if not pdf_link:
            meta = soup.find("meta", {"name": "citation_pdf_url"})
            if meta: pdf_link = meta['content']

        if pdf_link:
            # ä¸‹è¼‰æª”æ¡ˆ
            file_resp = requests.get(pdf_link, headers=get_headers(), timeout=15)
            if file_resp.status_code == 200:
                return "SUCCESS", f"{real_id}.pdf", file_resp.content, f"æˆåŠŸï¼(å°æ‡‰å…¬é–‹è™Ÿ: {real_id})"
            else:
                return "FAIL", None, None, "æ‰¾åˆ°é€£çµä½†ä¸‹è¼‰å¤±æ•—"
        else:
            return "NO_LINK", None, None, f"æ‰¾åˆ°ç¶²é  ({real_id}) ä½†æ²’æœ‰ PDF ä¸‹è¼‰é»"
            
    except Exception as e:
        return "ERROR", None, None, str(e)

# 2. æŒ‰éˆ•é‚è¼¯
if st.button("ğŸš€ å•Ÿå‹•çµ‚æ¥µæœå°‹"):
    if patent_ids:
        raw_list = [x.strip() for x in patent_ids.split('\n') if x.strip()]
        
        zip_buffer = io.BytesIO()
        results_log = []
        success_count = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()

        with zipfile.ZipFile(zip_buffer, "w") as zf:
            for i, pid in enumerate(raw_list):
                status_text.text(f"æ­£åœ¨åµæ¢èª¿æŸ¥ ({i+1}/{len(raw_list)}): {pid} ...")
                
                # åŸ·è¡Œæœå°‹
                code, filename, content, msg = get_pdf_data(pid)
                
                if code == "SUCCESS":
                    zf.writestr(filename, content)
                    success_count += 1
                    results_log.append(f"âœ… **{pid}** -> {msg}")
                elif code == "NOT_FOUND":
                    results_log.append(f"âŒ **{pid}**: Google æœå°‹ä¹Ÿæ‰¾ä¸åˆ°ï¼Œè«‹ç¢ºèªè™Ÿç¢¼ã€‚")
                else:
                    results_log.append(f"âš ï¸ **{pid}**: {msg}")
                
                progress_bar.progress((i + 1) / len(raw_list))
                # æœå°‹éœ€è¦ä¸€é»æ™‚é–“ä¼‘æ¯ï¼Œé¿å…è¢« Google æ‡·ç–‘
                time.sleep(random.uniform(2.0, 4.0))

        status_text.text("è™•ç†å®Œæˆï¼")
        st.divider()
        
        if success_count > 0:
            zip_buffer.seek(0)
            st.success(f"ğŸ‰ æˆåŠŸä¸‹è¼‰ {success_count} å€‹æª”æ¡ˆï¼")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰æ‰“åŒ…æª”æ¡ˆ (.zip)",
                data=zip_buffer,
                file_name="ultimate_patents.zip",
                mime="application/zip",
                type="primary"
            )
        
        with st.expander("æŸ¥çœ‹è©³ç´°åµæ¢å ±å‘Š", expanded=True):
            for log in results_log:
                st.markdown(log)
    else:
        st.warning("è«‹å…ˆè¼¸å…¥æ¡ˆè™Ÿ")
