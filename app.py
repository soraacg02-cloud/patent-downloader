import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import zipfile
import io
import random
import urllib.parse

st.set_page_config(page_title="å°ˆåˆ© PDF è¬èƒ½ä¸‹è¼‰å™¨ v4.0", page_icon="ğŸ¦¾")

st.title("ğŸ¦¾ Google Patents è¬èƒ½ä¸‹è¼‰å™¨ v4.0")
st.markdown("æ”¯æ´ **å…¬é–‹è™Ÿ** èˆ‡ **ç”³è«‹è™Ÿ** (è‡ªå‹•åæŸ¥å°æ‡‰å°ˆåˆ©)ã€‚")

# 1. ä½¿ç”¨è€…è¼¸å…¥å€
patent_ids = st.text_area(
    "åœ¨æ­¤è¼¸å…¥å°ˆåˆ©æ¡ˆè™Ÿ (ä¸€è¡Œä¸€å€‹)", 
    height=150, 
    placeholder="US20240088000A1 (å…¬é–‹è™Ÿ - æœ€å¿«)\n18/671705 (ç¾åœ‹ç”³è«‹è™Ÿ)\n2022-11738495 (ä¸­åœ‹ç”³è«‹è™Ÿ)"
)

# å½è£æ¨™é ­ (éš¨æ©Ÿåˆ‡æ›ï¼Œé™ä½è¢«é–æ©Ÿç‡)
def get_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept-Language": "en-US,en;q=0.9",
    }

def search_google_for_url(query):
    """
    ç•¶ç›´æ¥çŒœæ¸¬å¤±æ•—æ™‚ï¼Œåˆ©ç”¨ Google æœå°‹ä¾†æ‰¾çœŸæ­£çš„å°ˆåˆ©é é¢
    æŒ‡ä»¤: site:patents.google.com [æ¡ˆè™Ÿ]
    """
    # é™åˆ¶æœå°‹ç¯„åœåœ¨ patents.google.com
    search_query = f"site:patents.google.com {query}"
    google_search_url = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
    
    try:
        # æœå°‹æ™‚è¦ç¨å¾®ç­‰å¾…ï¼Œæ¨¡æ“¬äººé¡è¡Œç‚º
        time.sleep(random.uniform(1.0, 2.0))
        resp = requests.get(google_search_url, headers=get_headers(), timeout=10)
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, "html.parser")
            # åœ¨æœå°‹çµæœä¸­æ‰¾ç¬¬ä¸€å€‹é€£çµ
            # Google æœå°‹çµæœçš„é€£çµé€šå¸¸åœ¨ 'a' æ¨™ç±¤ä¸­ï¼Œä¸” href åŒ…å« 'patents.google.com/patent/'
            for link in soup.find_all('a', href=True):
                href = link['href']
                if "patents.google.com/patent/" in href:
                    # æŠ“åˆ°äº†ï¼é€™å°±æ˜¯ Google å¹«æˆ‘å€‘æ‰¾åˆ°çš„æ­£ç¢ºé é¢
                    # æœ‰æ™‚å€™é€£çµæœƒåŒ…å«å¤šé¤˜çš„ Google åƒæ•¸ï¼Œé€™è£¡åšå€‹ç°¡å–®æ¸…ç†
                    if "/url?q=" in href:
                        href = href.split("/url?q=")[1].split("&")[0]
                    return href
    except Exception as e:
        print(f"Search failed: {e}")
    return None

def get_pdf_data(patent_id):
    """
    ä¸»é‚è¼¯ï¼šå…ˆå˜—è©¦ç›´æ¥çŒœæ¸¬ -> å¤±æ•—å‰‡å˜—è©¦æœå°‹ -> ä¸‹è¼‰ PDF
    å›å‚³: (status, filename, content_bytes)
    """
    clean_id = patent_id.strip()
    # å˜—è©¦ 1: ç›´æ¥æ§‹é€ ç¶²å€ (æœ€å¿«ï¼Œé©åˆå…¬é–‹è™Ÿ)
    # æˆ‘å€‘å…ˆå‡è¨­å®ƒæ˜¯å…¬é–‹è™Ÿï¼Œä¸¦å˜—è©¦å»é™¤éæ³•å­—å…ƒ
    guess_id = clean_id.replace(" ", "").replace("-", "").replace("/", "")
    target_url = f"https://patents.google.com/patent/{guess_id}/en"
    
    # ç”¨ä¾†è¨˜éŒ„æœ€çµ‚æˆåŠŸçš„ç¶²å€
    final_url = None
    
    # å…ˆè©¦è©¦çœ‹ç›´æ¥é€£
    try:
        check = requests.get(target_url, headers=get_headers(), timeout=5)
        if check.status_code == 200:
            final_url = target_url
        elif check.status_code == 404:
            # 404 ä»£è¡¨ç›´æ¥çŒœæ¸¬å¤±æ•—ï¼Œé€™å¯èƒ½æ˜¯ã€Œç”³è«‹è™Ÿã€
            # å•Ÿå‹• B è¨ˆç•«ï¼šGoogle æœå°‹
            found_url = search_google_for_url(clean_id)
            if found_url:
                final_url = found_url
    except:
        # å¦‚æœç¶²è·¯å‡ºéŒ¯ï¼Œä¹Ÿå˜—è©¦æœå°‹çœ‹çœ‹
        found_url = search_google_for_url(clean_id)
        if found_url:
            final_url = found_url

    # å¦‚æœç¶“éä¸€ç•ªæŠ˜é¨°é‚„æ˜¯æ²’ç¶²å€ï¼Œå®£å‘Šå¤±æ•—
    if not final_url:
        return "NOT_FOUND", None, None

    # å˜—è©¦å¾æœ€çµ‚ç¶²å€æŠ“ PDF é€£çµ
    try:
        resp = requests.get(final_url, headers=get_headers(), timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        
        pdf_link = None
        # æ‰¾é€£çµ
        for link in soup.find_all('a', href=True):
            if link['href'].endswith('.pdf'):
                pdf_link = link['href']
                break
        
        # æ‰¾ Meta tag
        if not pdf_link:
            meta = soup.find("meta", {"name": "citation_pdf_url"})
            if meta: pdf_link = meta['content']
            
        if pdf_link:
            # ä¸‹è¼‰æª”æ¡ˆå…§å®¹
            pdf_resp = requests.get(pdf_link, headers=get_headers(), timeout=15)
            if pdf_resp.status_code == 200:
                # ç‚ºäº†æª”åæ¼‚äº®ï¼Œæˆ‘å€‘è©¦è‘—å¾ç¶²å€è§£æçœŸæ­£çš„å°ˆåˆ©è™Ÿ (ä¾‹å¦‚å¾ URL ä¸­æŠ“å– US123456)
                real_id = final_url.split("/patent/")[-1].split("/")[0]
                return "SUCCESS", f"{real_id}.pdf", pdf_resp.content
            else:
                return "DOWNLOAD_FAIL", None, None
        else:
            return "NO_PDF_LINK", None, None
            
    except Exception as e:
        return "ERROR", None, None

# 2. åŸ·è¡Œé‚è¼¯
if st.button("ğŸš€ å•Ÿå‹•è¬èƒ½æœå°‹"):
    if patent_ids:
        raw_list = [x.strip() for x in patent_ids.split('\n') if x.strip()]
        
        zip_buffer = io.BytesIO()
        results_log = []
        success_count = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()

        with zipfile.ZipFile(zip_buffer, "w") as zf:
            for i, pid in enumerate(raw_list):
                status_text.text(f"æ­£åœ¨åˆ†æ ({i+1}/{len(raw_list)}): {pid} ...")
                
                # å‘¼å«ä¸»é‚è¼¯
                status, filename, content = get_pdf_data(pid)
                
                if status == "SUCCESS":
                    zf.writestr(filename, content)
                    success_count += 1
                    results_log.append(f"âœ… **{pid}** -> æ‰¾åˆ° `{filename}` (æˆåŠŸ)")
                elif status == "NOT_FOUND":
                    results_log.append(f"âŒ **{pid}**: æœå°‹ä¸åˆ°å°æ‡‰å°ˆåˆ© (è«‹ç¢ºèªè™Ÿç¢¼)")
                elif status == "NO_PDF_LINK":
                    results_log.append(f"âš ï¸ **{pid}**: æ‰¾åˆ°å°ˆåˆ©é é¢ä½†æ²’æœ‰ PDF ä¸‹è¼‰é»")
                else:
                    results_log.append(f"âš ï¸ **{pid}**: ä¸‹è¼‰éç¨‹ç™¼ç”ŸéŒ¯èª¤")
                
                progress_bar.progress((i + 1) / len(raw_list))
                # é‡è¦ï¼šå› ç‚ºç”¨äº† Google æœå°‹ï¼Œå¿…é ˆå¤šä¼‘æ¯ä¸€ä¸‹é¿å…è¢«é– IP
                time.sleep(random.uniform(2.0, 4.0))

        status_text.text("è™•ç†å®Œæˆï¼")
        st.divider()
        
        if success_count > 0:
            zip_buffer.seek(0)
            st.success(f"ğŸ‰ æˆåŠŸä¸‹è¼‰ {success_count} ç­†å°ˆåˆ©ï¼")
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰æ‰“åŒ…æª”æ¡ˆ (.zip)",
                data=zip_buffer,
                file_name="smart_patents_bundle.zip",
                mime="application/zip",
                type="primary"
            )
        else:
            st.error("æ²’æœ‰æˆåŠŸä¸‹è¼‰ä»»ä½•æª”æ¡ˆã€‚")

        with st.expander("æŸ¥çœ‹è©³ç´°å ±å‘Š", expanded=True):
            for log in results_log:
                st.markdown(log)
    else:
        st.warning("è«‹è¼¸å…¥æ¡ˆè™Ÿ")
